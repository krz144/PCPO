{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a441c26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "069fe361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harris(filepath, blockSize=2, ksize=3, k=0.004):\n",
    "    \"\"\"\n",
    "    # blockSize (np. 2) - Jest to wielkość sąsiedztwa brana pod uwagę przy wykrywaniu narożników\n",
    "    blockSize - It is the size of neighbourhood considered for corner detection\n",
    "    # ksize (np. 3) - Parametr wielkości maksi używanej pochodnej Sobela.\n",
    "    ksize - Aperture parameter of the Sobel derivative used.\n",
    "    # k (np. 0.004) - Parametr wolny w równaniu detektora Harris (0.004 – 0.006)\n",
    "    k - Harris detector free parameter in the equation.\n",
    "    \n",
    "    https://docs.opencv.org/3.4/dc/d0d/tutorial_py_features_harris.html\n",
    "    \"\"\"\n",
    "    img = cv2.imread(filepath)\n",
    "    img_grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_grey = np.float32(img_grey)\n",
    "    harris = cv2.cornerHarris(img_grey, blockSize, ksize, k)\n",
    "    img[harris > 0.1 * harris.max()] = [0, 0, 255]\n",
    "    # co tu się dzieje? co oznacza harris max? zo zwraca harris? czemu to nie jest wyjaśniane.... jprdlsvd kjbsdbkgfjjkdfa\n",
    "    cv2.imshow('Punkty wykryte detektorem Harrisa', img)\n",
    "    if cv2.waitKey(0) & 0xff == 27:\n",
    "        cv2.destroyAllWindows()\n",
    "    return img  #, harris? jak dostac wsp... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ded52a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harris_extended(filepath, blockSize=2, ksize=3, k=0.004, display_result=False):\n",
    "    \"\"\"\n",
    "    # blockSize (np. 2) - Jest to wielkość sąsiedztwa brana pod uwagę przy wykrywaniu narożników\n",
    "    blockSize - It is the size of neighbourhood considered for corner detection\n",
    "    # ksize (np. 3) - Parametr wielkości maksi używanej pochodnej Sobela.\n",
    "    ksize - Aperture parameter of the Sobel derivative used.\n",
    "    # k (np. 0.004) - Parametr wolny w równaniu detektora Harris (0.004 – 0.006)\n",
    "    k - Harris detector free parameter in the equation.\n",
    "    \n",
    "    https://docs.opencv.org/3.4/dc/d0d/tutorial_py_features_harris.html\n",
    "    \"\"\"\n",
    "    # Konwersja obrazu do postaci zdjęcia w odcieniach szarości\n",
    "    zdjecie = cv2.imread(filepath)\n",
    "    szareZdjecie = cv2.cvtColor(zdjecie, cv2.COLOR_BGR2GRAY) \n",
    "\n",
    "    # find Harris corners\n",
    "    szareZdjecie = np.float32(szareZdjecie)\n",
    "    harris = cv2.cornerHarris(szareZdjecie, blockSize, ksize, k)\n",
    "    harris = cv2.dilate(harris, None)\n",
    "    ret, dst = cv2.threshold(harris, 0.01 * harris.max(), 255, 0)\n",
    "    dst = np.uint8(dst)\n",
    "\n",
    "    # find centroids\n",
    "    ret, labels, stats, centroids = cv2.connectedComponentsWithStats(dst)\n",
    "\n",
    "    # define the criteria to stop and refine the corners\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.001)\n",
    "    corners = cv2.cornerSubPix(szareZdjecie, np.float32(centroids), (5, 5), (-1, -1), criteria)\n",
    "\n",
    "    # Now draw them\n",
    "    res = np.hstack((centroids, corners))\n",
    "    res = np.int0(res)\n",
    "    zdjecie[res[:, 1], res[:, 0]] = [0, 0, 255] # image = cv2.circle(image, (int(v[0]),int(v[1])), radius=2, color=(0, 0, 255), thickness=5)\n",
    "    zdjecie[res[:, 3], res[:, 2]] = [0, 255, 0]\n",
    "\n",
    "    if display_result:\n",
    "        cv2.imshow('Punkty wykryte detektorem Harrisa', zdjecie)\n",
    "        if cv2.waitKey(0) & 0xff == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "    return corners, zdjecie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d57b531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harris_extended_in_array(input_array, blockSize=2, ksize=3, k=0.004, threshold=0.01):\n",
    "    \"\"\"\n",
    "    input_array - np.ndarray(dtype=\"float32\") reprezentująca zdjęcie w odcieniach szarości\n",
    "    # blockSize (np. 2) - Jest to wielkość sąsiedztwa brana pod uwagę przy wykrywaniu narożników\n",
    "    blockSize - It is the size of neighbourhood considered for corner detection\n",
    "    # ksize (np. 3) - Parametr wielkości maksi używanej pochodnej Sobela.\n",
    "    ksize - Aperture parameter of the Sobel derivative used.\n",
    "    # k (np. 0.004) - Parametr wolny w równaniu detektora Harris (0.004 – 0.006)\n",
    "    k - Harris detector free parameter in the equation.\n",
    "    # threshold - odsyłam do dokumentacji\n",
    "    \n",
    "    https://docs.opencv.org/3.4/dc/d0d/tutorial_py_features_harris.html\n",
    "    \"\"\"\n",
    "    # find Harris corners\n",
    "    harris = cv2.cornerHarris(input_array, blockSize, ksize, k)\n",
    "    harris = cv2.dilate(harris, None)\n",
    "    ret, dst = cv2.threshold(harris, threshold * harris.max(), 255, 0)\n",
    "    dst = np.uint8(dst)\n",
    "\n",
    "    # find centroids\n",
    "    ret, labels, stats, centroids = cv2.connectedComponentsWithStats(dst)\n",
    "\n",
    "    # define the criteria to stop and refine the corners\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.001)\n",
    "    corners = cv2.cornerSubPix(input_array, np.float32(centroids), (5, 5), (-1, -1), criteria)\n",
    "\n",
    "    return corners, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5624c711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_stock(filepath):\n",
    "    img = cv2.imread(filepath)\n",
    "    img_grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # Inicjalizacja detektora FAST z wartościami domyślnymi funkcji\n",
    "    fast = cv2.FastFeatureDetector.create()\n",
    "    # Wykrycie punktów charakterystycznych (keypoints)\n",
    "    kp = fast.detect(img_grey, None)\n",
    "    # Narysowanie punktów charakterystycznych (keypoints)\n",
    "    img2 = cv2.drawKeypoints(img_grey, kp, None, color=(255, 0, 0))\n",
    "    # Wyświetlenie parametrów funkcji\n",
    "    print(\"Threshold: \", fast.getThreshold())\n",
    "    print(\"nonmaxSuppression: \", fast.getNonmaxSuppression())\n",
    "    print(\"neighborhood: \", fast.getType())\n",
    "    print(\"Total Keypoints without nonmaxSuppression: \", len(kp))\n",
    "    cv2.imshow('Punkty wykryte algorytmem Fast', img2)\n",
    "    if cv2.waitKey(0) & 0xff == 27:\n",
    "        cv2.destroyAllWindows()\n",
    "    return img2 #, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16d00956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast(filepath, threshold=10, nonmaxSuppresion=True, type=cv2.FAST_FEATURE_DETECTOR_TYPE_9_16):\n",
    "    \"\"\"\n",
    "    FAST: Features from  Accelerated  Segment  Test\n",
    "    cv2.FAST_FEATURE_DETECTOR_TYPE_9_16, cv2.FAST_FEATURE_DETECTOR_TYPE_5_8, cv2.FAST_FEATURE_DETECTOR_TYPE_7_12\n",
    "    \"\"\"\n",
    "    img = cv2.imread(filepath)\n",
    "    img_grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    fast = cv2.FastFeatureDetector.create(threshold, nonmaxSuppresion, type)\n",
    "    keypoints = fast.detect(img_grey, None)\n",
    "    img2 = cv2.drawKeypoints(img_grey, keypoints, None, color=(255, 0, 0))\n",
    "    # cv2.imshow('Punkty wykryte algorytmem Fast', img2)\n",
    "    if cv2.waitKey(0) & 0xff == 27:\n",
    "        cv2.destroyAllWindows()\n",
    "    return img2, keypoints # . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d3b038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sift(img_filepath, nfeatures=0, nOctaveLayers=3, contrastThreshold=0.04, edgeThreshold=10, sigma=1.6, display_result=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "        img_filepath     filepath to RGB image\n",
    "        nfeatures\tThe number of best features to retain. The features are ranked by their scores (measured in SIFT algorithm as the local contrast)\n",
    "        nOctaveLayers\tThe number of layers in each octave. 3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.\n",
    "        contrastThreshold\tThe contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions. The larger the threshold, the less features are produced by the detector.\n",
    "        edgeThreshold\tThe threshold used to filter out edge-like features. Note that the its meaning is different from the contrastThreshold, i.e. the larger the edgeThreshold, the less features are filtered out (more features are retained).\n",
    "        sigma\tThe sigma of the Gaussian applied to the input image at the octave #0. If your image is captured with a weak camera with soft lenses, you might want to reduce the number.\n",
    "    Note\n",
    "        The contrast threshold will be divided by nOctaveLayers when the filtering is applied. When nOctaveLayers is set to default (3) and if you want to use the value used in D. Lowe paper, 0.03, set this argument to 0.09.\n",
    "    \n",
    "    # https://docs.opencv.org/3.4/da/df5/tutorial_py_sift_intro.html\n",
    "    # https://docs.opencv.org/4.x/d7/d60/classcv_1_1SIFT.html\n",
    "    \"\"\"\n",
    "    img = cv2.imread(img_filepath)\n",
    "    img_grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.SIFT_create(nfeatures, nOctaveLayers, contrastThreshold, edgeThreshold, sigma)\n",
    "    keypoints = sift.detect(img_grey, None)\n",
    "    zdjecie = cv2.drawKeypoints(img, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    if display_result:\n",
    "        cv2.imshow(f\"Detektor SIFT, {nfeatures=} {nOctaveLayers=} {contrastThreshold=} {edgeThreshold=} {sigma=}\", zdjecie)\n",
    "        if cv2.waitKey(0) & 0xff == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "    return zdjecie, keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c938806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sift_in_array(input_array, nfeatures=0, nOctaveLayers=3, contrastThreshold=0.04, edgeThreshold=10, sigma=1.6, display_result=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "        input_array     np.ndarray represening an image in greyscale\n",
    "        nfeatures\tThe number of best features to retain. The features are ranked by their scores (measured in SIFT algorithm as the local contrast)\n",
    "        nOctaveLayers\tThe number of layers in each octave. 3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.\n",
    "        contrastThreshold\tThe contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions. The larger the threshold, the less features are produced by the detector.\n",
    "        edgeThreshold\tThe threshold used to filter out edge-like features. Note that the its meaning is different from the contrastThreshold, i.e. the larger the edgeThreshold, the less features are filtered out (more features are retained).\n",
    "        sigma\tThe sigma of the Gaussian applied to the input image at the octave #0. If your image is captured with a weak camera with soft lenses, you might want to reduce the number.\n",
    "    Note\n",
    "        The contrast threshold will be divided by nOctaveLayers when the filtering is applied. When nOctaveLayers is set to default (3) and if you want to use the value used in D. Lowe paper, 0.03, set this argument to 0.09.\n",
    "    \n",
    "    # https://docs.opencv.org/3.4/da/df5/tutorial_py_sift_intro.html\n",
    "    # https://docs.opencv.org/4.x/d7/d60/classcv_1_1SIFT.html\n",
    "    \"\"\"\n",
    "    sift = cv2.SIFT_create(nfeatures, nOctaveLayers, contrastThreshold, edgeThreshold, sigma)\n",
    "    keypoints = sift.detect(input_array, None)\n",
    "    just_coords = [p.pt for p in kp]\n",
    "    zdjecie = cv2.drawKeypoints(input_array, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    if display_result:\n",
    "        cv2.imshow(f\"Detektor SIFT, {nfeatures=} {nOctaveLayers=} {contrastThreshold=} {edgeThreshold=} {sigma=}\", zdjecie)\n",
    "        if cv2.waitKey(0) & 0xff == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "    return keypoints, just_coords, zdjecie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c07cd4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mouse_callback_function(event, x, y, flags, param):\n",
    "    global pixel_coords_4_points  # lista globalna\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:  # cv2.EVENT_LBUTTONDOWN EVENT_LBUTTONDBLCLK\n",
    "        pixel_coords_4_points.append((x, y))\n",
    "        print(pixel_coords_4_points)\n",
    "    if len(pixel_coords_4_points)==4:\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a66fefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warpPoints(list_of_points, transformation_matrix):\n",
    "    \"\"\"\n",
    "    warpPerspective for list of 2D points\n",
    "    Inspired by https://docs.opencv.org/3.4/da/d54/group__imgproc__transform.html#gaf73673a7e8e18ec6963e3774e6a94b87\n",
    "\n",
    "    Args:\n",
    "        list_of_points (list): list of 2-element tuples eg.: [(0,0), (-2,-4), (7,7)]\n",
    "        transformation_matrix (np.ndarray): matrix used for transoformation according to equation in https://docs.opencv.org/3.4/da/d54/group__imgproc__transform.html#gaf73673a7e8e18ec6963e3774e6a94b87\n",
    "\n",
    "    Returns:\n",
    "        list_of_transformed_points\n",
    "    \"\"\"\n",
    "    list_of_transformed_points = []\n",
    "    for p in list_of_points:\n",
    "        tp = ((transformation_matrix[0,0]*p[0] + transformation_matrix[0,1]*p[1] + transformation_matrix[0,2])/(transformation_matrix[2,0]*p[0] + transformation_matrix[2,1]*p[1] + transformation_matrix[2,2]),\n",
    "              (transformation_matrix[1,0]*p[0] + transformation_matrix[1,1]*p[1] + transformation_matrix[1,2])/(transformation_matrix[2,0]*p[0] + transformation_matrix[2,1]*p[1] + transformation_matrix[2,2]))\n",
    "        list_of_transformed_points.append(tp)\n",
    "    return list_of_transformed_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "c6d3859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corresponding_points(img_path, scale_percent=15, gridsize=20, squares_w=7, squares_h=7, square_side=4.8, snap_threshold=1.2, haris_threshold=0.01, show_detected_pts=True, show_filtered_detected_pts=True, show_grid=True):\n",
    "    \"\"\"Nie chce mi się pisać docstringa.\n",
    "    Przed wywołaniem musi być zainicjalizowana globalna lista pixel_coords_4_points = [] \n",
    "\n",
    "    Args:\n",
    "        img_path (str): Image filepath.\n",
    "        scale_percent (float): Img resize percent.\n",
    "        gridsize (int, optional): Defaults to 20. Ile oczek siatki w górę, w dół, w prawo, w lewo od punktu centralnego siatki.\n",
    "        squares_w (int, optional): Defaults to 7. Szerokość w kwadratach prostokąta klikanego (klikanych czterech narożników).\n",
    "        squares_h (int, optional): Defaults to 7. Wysokość w kwadratach prostokąta klikanego (klikanych czterech narożników).\n",
    "        square_side (float, optional): Defaults to 4.8. Długość boku jednego kwadratu (pola szachownicy).\n",
    "        snap_threshold (float, optional): Defaults to 1.2.\n",
    "        haris_threshold (float, optional): Defaults to 0.01.\n",
    "\n",
    "    Returns:\n",
    "        (List, List): (our_xyz_points_transformed, closest_detected_points)\n",
    "    \"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "    img_grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    width = int(img.shape[1] * scale_percent / 100)\n",
    "    height = int(img.shape[0] * scale_percent / 100)\n",
    "    img_resized = cv2.resize(img, (width, height), interpolation=cv2.INTER_CUBIC)\n",
    "    img_grey_resized = cv2.resize(img_grey, (width, height), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "\n",
    "    # pixel_coords_4_points = [] \n",
    "    def mouse_callback_function(event, x, y, flags, param):\n",
    "        global pixel_coords_4_points  # lista globalna\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            pixel_coords_4_points.append((x, y))\n",
    "            print(pixel_coords_4_points)\n",
    "        if len(pixel_coords_4_points)==4:\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "    # Klikamy cztery narożniki tworzące prostokąt squares_w x squares_h (default 7x7) w ustalonej kolejkności: LU RU RD LD\n",
    "    # pixel_coords_4_points = [] \n",
    "    cv2.namedWindow(\"Click four corners\")\n",
    "    cv2.setMouseCallback(\"Click four corners\", mouse_callback_function)\n",
    "    cv2.imshow(\"Click four corners\", img_resized)\n",
    "    if cv2.waitKey(0):\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    # Kliknięte współrzędne pikselowe zapisywane są w liście pixel_coords_4_points\n",
    "    global pixel_coords_4_points\n",
    "    pixel_coords_4_points = np.array(pixel_coords_4_points, dtype=np.dtype('float32'))\n",
    "\n",
    "    # Tworzymy nasz układ współrzędnych (wybieramy cztery punkty odpowiadające klikniętym). (0,0) w lewym górnym kliniętym narożniku.\n",
    "    our_coords_4_points = [(0, 0), (square_side*squares_w, 0), (square_side*squares_w, square_side*squares_h), (0, square_side*squares_h)]\n",
    "    our_coords_4_points = np.array(our_coords_4_points, dtype=np.dtype('float32'))\n",
    "\n",
    "    # Macierz do transformacji z układu wsp. pikselowych do naszego układu wsp.\n",
    "    macierz_transformacji = cv2.getPerspectiveTransform(pixel_coords_4_points, our_coords_4_points)\n",
    "\n",
    "    # Macierz do odwrotnej transformacji (z naszego układu wsp. do układu wsp. pikselowych)\n",
    "    macierz_odwrotna = np.linalg.inv(macierz_transformacji)\n",
    "\n",
    "    # Robimy siatkę/listę punktów w naszym układzie współrzędnych\n",
    "    x = np.arange(-gridsize*4.8, gridsize*4.8, square_side)\n",
    "    y = np.arange(-gridsize*4.8, gridsize*4.8, square_side)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    our_xyz_points = np.hstack([np.expand_dims(xx.ravel(),axis=0).T, np.expand_dims(yy.ravel(),axis=0).T, np.expand_dims(np.repeat(1, yy.size),axis=0).T]) \n",
    "    our_xy_points = our_xyz_points[:, :2]\n",
    "\n",
    "    # Wykrycie punktów narożnych detektorem harrisa\n",
    "    detected_points, centroids = harris_extended_in_array(img_grey_resized, threshold=haris_threshold)\n",
    "\n",
    "    # Wyświetlenie wykrytych narożników\n",
    "    if show_detected_pts:\n",
    "        image = copy.deepcopy(img_resized)\n",
    "        for v in detected_points:\n",
    "            image = cv2.circle(image, (int(v[0]),int(v[1])), radius=0, color=(0, 0, 255), thickness=5)\n",
    "        cv2.imshow('Detected points (harris detector)', image)\n",
    "        if cv2.waitKey(0) & 0xff == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "    # Transformacja wykrytych narożników do naszego układu współrzędnych\n",
    "    detected_points_in_our_coords = warpPoints(detected_points, macierz_transformacji)\n",
    "\n",
    "    # Dla każdego punktu z naszej siatki wykrycie najbliższego punktu wykrytego detektorem \n",
    "    closest_detected_points_in_our_coords = []\n",
    "    our_xy_points_2 = []\n",
    "    for p in our_xy_points:\n",
    "        distances = np.linalg.norm(detected_points_in_our_coords - p, axis=1)\n",
    "        min_index = np.argmin(distances)\n",
    "        if distances[min_index] <= snap_threshold:\n",
    "            closest_detected_points_in_our_coords.append(detected_points_in_our_coords[min_index])\n",
    "            our_xy_points_2.append(p)\n",
    "\n",
    "    # Transformacja punktów z detektora (najbliższych siatce) do ich orginalnego pikselowego układu współrzędnych zdjęcia\n",
    "    closest_detected_points = warpPoints(closest_detected_points_in_our_coords, macierz_odwrotna)\n",
    "    \n",
    "    # Wyświetlenie odfiltrowanych punktów z detektora\n",
    "    if show_filtered_detected_pts:\n",
    "        image = copy.deepcopy(img_resized)\n",
    "        for v in closest_detected_points:\n",
    "            image = cv2.circle(image, (int(v[0]),int(v[1])), radius=0, color=(0, 0, 0), thickness=5)\n",
    "        cv2.imshow('closest detected points (harris detector)', image)\n",
    "        if cv2.waitKey(0) & 0xff == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "    # Transformacja siatki z naszego układu do układu pikselowego zdjęcia\n",
    "    our_xy_points_transformed = warpPoints(our_xy_points_2, macierz_odwrotna)\n",
    "\n",
    "    # Wyświetlenie przetransformowanych punktów grid\n",
    "    if show_grid:\n",
    "        image = copy.deepcopy(img_resized)\n",
    "        for v in our_xy_points_transformed:\n",
    "            image = cv2.circle(image, (int(v[0]),int(v[1])), radius=0, color=(150, 0, 50), thickness=5)\n",
    "        cv2.imshow('our xy points transformed', image)\n",
    "        if cv2.waitKey(0) & 0xff == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "    img_grey_resized_size = img_grey_resized.shape[::-1]\n",
    "    our_xyz_points_transformed = np.hstack((np.array(our_xy_points_transformed), np.zeros(shape=(len(our_xy_points_transformed),1)))).astype('float32')  #.tolist()\n",
    "    # return our_xyz_points_transformed, closest_detected_points, img_grey_resized_size\n",
    "\n",
    "    # TODO nwm co sie dzieje... CO TRZEBA ZWRACAĆ?\n",
    "    our_points = []\n",
    "    for pair in our_xy_points_2:\n",
    "        our_points.append((pair[0], pair[1], 0))\n",
    "    our_points = np.array(our_points, dtype=np.dtype(\"float32\"))\n",
    "    detected_points = np.expand_dims(np.array(closest_detected_points, dtype=np.dtype(\"float32\")), axis=1)\n",
    "    return our_points, detected_points, img_grey_resized_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa2ed632",
   "metadata": {},
   "source": [
    "# Dane wejściowe (folder ze zdjęciami) ```*można pominąć tę sekcję, a na końcu usunąć*```\n",
    "i przygotowanie list z macierzami reprezentującymi zdjęcia (rgb/grey, orginalny_rozmiar/resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c11d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytanie z folderu obrazków w rgb i obrazków skali szarości (jako macierze) do list: images i images_grey\n",
    "# board_images_folder = 'D:\\\\STUDIA_6_SEM\\\\PCPO\\\\zdj_do_kalibracji_1'\n",
    "board_images_folder = r\"C:\\SEM6\\PCPO\\p9\\images\"\n",
    "files = [f for f in listdir(board_images_folder) if isfile(join(board_images_folder, f))]\n",
    "images = []\n",
    "images_grey = []\n",
    "for filename in files:\n",
    "    filepath = rf\"{board_images_folder}\\\\{filename}\"\n",
    "    img = cv2.imread(filepath)\n",
    "    img_grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    images.append(img)\n",
    "    images_grey.append(img_grey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e789d9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize obrazów i wczytanie ich do list: images_resized i images_grey_resized\n",
    "images_resized = []\n",
    "images_grey_resized = []\n",
    "scale_percent = 15 # percent of original size\n",
    "for img_grey, img in zip(images_grey, images):\n",
    "    width = int(img.shape[1] * scale_percent / 100)\n",
    "    height = int(img.shape[0] * scale_percent / 100)\n",
    "    img_resized = cv2.resize(img, (width, height), interpolation=cv2.INTER_CUBIC)\n",
    "    img_grey_resized = cv2.resize(img_grey, (width, height), interpolation=cv2.INTER_CUBIC)\n",
    "    images_grey_resized.append(img_grey_resized)\n",
    "    images_resized.append(img_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e676297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyświetlenie obrazków z wybranej listy\n",
    "lista = images_resized\n",
    "for i, img in enumerate(lista):\n",
    "    cv2.imshow(f'image {i+1} out of {len(lista)}', img)\n",
    "    if cv2.waitKey(0) & 0xff == 27:\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "134bc2da",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "c973874f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\SEM6\\\\PCPO\\\\p9\\\\images_2\\\\MicrosoftTeams-image (1).png',\n",
       " 'C:\\\\SEM6\\\\PCPO\\\\p9\\\\images_2\\\\MicrosoftTeams-image (10).png',\n",
       " 'C:\\\\SEM6\\\\PCPO\\\\p9\\\\images_2\\\\MicrosoftTeams-image (11).png',\n",
       " 'C:\\\\SEM6\\\\PCPO\\\\p9\\\\images_2\\\\MicrosoftTeams-image (2).png',\n",
       " 'C:\\\\SEM6\\\\PCPO\\\\p9\\\\images_2\\\\MicrosoftTeams-image (3).png',\n",
       " 'C:\\\\SEM6\\\\PCPO\\\\p9\\\\images_2\\\\MicrosoftTeams-image (4).png',\n",
       " 'C:\\\\SEM6\\\\PCPO\\\\p9\\\\images_2\\\\MicrosoftTeams-image (5).png',\n",
       " 'C:\\\\SEM6\\\\PCPO\\\\p9\\\\images_2\\\\MicrosoftTeams-image (6).png',\n",
       " 'C:\\\\SEM6\\\\PCPO\\\\p9\\\\images_2\\\\MicrosoftTeams-image (7).png',\n",
       " 'C:\\\\SEM6\\\\PCPO\\\\p9\\\\images_2\\\\MicrosoftTeams-image (8).png',\n",
       " 'C:\\\\SEM6\\\\PCPO\\\\p9\\\\images_2\\\\MicrosoftTeams-image (9).png',\n",
       " 'C:\\\\SEM6\\\\PCPO\\\\p9\\\\images_2\\\\MicrosoftTeams-image.png']"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "images = glob.glob(r\"C:\\SEM6\\PCPO\\p9\\images_2\\\\*.png\")\n",
    "results_folder = r\"C:\\SEM6\\PCPO\\p9\\images_2\\undistorted\\\\\"\n",
    "# images = glob.glob(r\"C:\\SEM6\\PCPO\\p9\\images\\\\*.jpg\")\n",
    "# results_folder = r\"C:\\SEM6\\PCPO\\p9\\images\\undistorted\\\\\"\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c714c68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(411, 296)]\n",
      "[(411, 296), (929, 297)]\n",
      "[(411, 296), (929, 297), (892, 963)]\n",
      "[(411, 296), (929, 297), (892, 963), (472, 941)]\n",
      "[(515, 389)]\n",
      "[(515, 389), (752, 535)]\n",
      "[(515, 389), (752, 535), (764, 1098)]\n",
      "[(515, 389), (752, 535), (764, 1098), (496, 1145)]\n",
      "[(267, 384)]\n",
      "[(267, 384), (671, 414)]\n",
      "[(267, 384), (671, 414), (597, 1293)]\n",
      "[(267, 384), (671, 414), (597, 1293), (284, 1049)]\n",
      "[(488, 199)]\n",
      "[(488, 199), (860, 153)]\n",
      "[(488, 199), (860, 153), (854, 889)]\n",
      "[(488, 199), (860, 153), (854, 889), (499, 797)]\n",
      "[(553, 140)]\n",
      "[(553, 140), (924, 195)]\n",
      "[(553, 140), (924, 195), (919, 806)]\n",
      "[(553, 140), (924, 195), (919, 806), (575, 922)]\n",
      "[(268, 237)]\n",
      "[(268, 237), (704, 203)]\n",
      "[(268, 237), (704, 203), (779, 878)]\n",
      "[(268, 237), (704, 203), (779, 878), (278, 915)]\n",
      "[(302, 416)]\n",
      "[(302, 416), (666, 472)]\n",
      "[(302, 416), (666, 472), (661, 1060)]\n",
      "[(302, 416), (666, 472), (661, 1060), (311, 1193)]\n",
      "[(595, 205)]\n",
      "[(595, 205), (1004, 201)]\n",
      "[(595, 205), (1004, 201), (1060, 871)]\n",
      "[(595, 205), (1004, 201), (1060, 871), (514, 841)]\n",
      "[(463, 163)]\n",
      "[(463, 163), (932, 177)]\n",
      "[(463, 163), (932, 177), (922, 866)]\n",
      "[(463, 163), (932, 177), (922, 866), (470, 883)]\n",
      "[(270, 434)]\n",
      "[(270, 434), (769, 432)]\n",
      "[(270, 434), (769, 432), (778, 1163)]\n",
      "[(270, 434), (769, 432), (778, 1163), (297, 1192)]\n",
      "[(227, 522)]\n",
      "[(227, 522), (747, 546)]\n",
      "[(227, 522), (747, 546), (695, 1233)]\n",
      "[(227, 522), (747, 546), (695, 1233), (262, 1190)]\n",
      "[(315, 448)]\n",
      "[(315, 448), (675, 402)]\n",
      "[(315, 448), (675, 402), (683, 1144)]\n",
      "[(315, 448), (675, 402), (683, 1144), (338, 1046)]\n"
     ]
    }
   ],
   "source": [
    "objpoints = []\n",
    "imgpoints = []\n",
    "for img_path in images:\n",
    "    pixel_coords_4_points = [] \n",
    "    our, detected, size = get_corresponding_points(img_path, scale_percent=70, gridsize=100, squares_w=15, squares_h=23)\n",
    "    objpoints.append(our)\n",
    "    imgpoints.append(detected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "643fdfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166, 3)\n",
      "(145, 3)\n",
      "(155, 3)\n",
      "(142, 3)\n",
      "(100, 3)\n",
      "(95, 3)\n",
      "(75, 3)\n",
      "(119, 3)\n",
      "(144, 3)\n",
      "(123, 3)\n",
      "(141, 3)\n",
      "(84, 3)\n",
      "(166, 1, 2)\n",
      "(145, 1, 2)\n",
      "(155, 1, 2)\n",
      "(142, 1, 2)\n",
      "(100, 1, 2)\n",
      "(95, 1, 2)\n",
      "(75, 1, 2)\n",
      "(119, 1, 2)\n",
      "(144, 1, 2)\n",
      "(123, 1, 2)\n",
      "(141, 1, 2)\n",
      "(84, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "for array in objpoints:\n",
    "    print(array.shape)\n",
    "for array in imgpoints:\n",
    "    print(array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2e732c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.5588502364509678,\n",
       " array([[1.11688774e+03, 0.00000000e+00, 6.19654247e+02],\n",
       "        [0.00000000e+00, 1.11131557e+03, 6.36953524e+02],\n",
       "        [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]]),\n",
       " array([[-0.22454995,  0.4165418 ,  0.00621792, -0.00659081, -0.29873302]]),\n",
       " (array([[ 0.38897692],\n",
       "         [ 0.15845153],\n",
       "         [-0.0050568 ]]),\n",
       "  array([[-0.28136919],\n",
       "         [-0.91968708],\n",
       "         [ 0.14474734]]),\n",
       "  array([[0.2141417 ],\n",
       "         [0.69949942],\n",
       "         [0.14601798]]),\n",
       "  array([[0.20786007],\n",
       "         [0.63458022],\n",
       "         [0.05901014]]),\n",
       "  array([[ 0.18452532],\n",
       "         [-0.55585404],\n",
       "         [-0.08277803]]),\n",
       "  array([[-0.18833798],\n",
       "         [-0.01282422],\n",
       "         [-0.07958083]]),\n",
       "  array([[ 0.00964902],\n",
       "         [-0.75938252],\n",
       "         [-0.00250752]]),\n",
       "  array([[-0.35537701],\n",
       "         [ 0.14219347],\n",
       "         [ 0.05779587]]),\n",
       "  array([[ 0.12318729],\n",
       "         [-0.04598486],\n",
       "         [-0.00707828]]),\n",
       "  array([[ 0.02126299],\n",
       "         [-0.07645614],\n",
       "         [-0.02059268]]),\n",
       "  array([[0.22079814],\n",
       "         [0.08668516],\n",
       "         [0.04838921]]),\n",
       "  array([[ 0.05724674],\n",
       "         [ 0.5491793 ],\n",
       "         [-0.00333737]])),\n",
       " (array([[-29.62730636],\n",
       "         [-48.57235715],\n",
       "         [156.20989122]]),\n",
       "  array([[-16.32814991],\n",
       "         [-39.04702602],\n",
       "         [175.97938903]]),\n",
       "  array([[-49.98354704],\n",
       "         [-36.88499166],\n",
       "         [157.03814076]]),\n",
       "  array([[-23.52558934],\n",
       "         [-77.58365395],\n",
       "         [191.99077831]]),\n",
       "  array([[ -9.08793042],\n",
       "         [-67.21220822],\n",
       "         [144.76542846]]),\n",
       "  array([[-57.91719089],\n",
       "         [-67.4903294 ],\n",
       "         [180.1167019 ]]),\n",
       "  array([[-44.05384697],\n",
       "         [-30.56005763],\n",
       "         [154.90786178]]),\n",
       "  array([[ -5.40651722],\n",
       "         [-76.23430319],\n",
       "         [190.08302197]]),\n",
       "  array([[-23.5544154 ],\n",
       "         [-71.36413101],\n",
       "         [161.9989114 ]]),\n",
       "  array([[-49.73117958],\n",
       "         [-28.93183788],\n",
       "         [157.31456284]]),\n",
       "  array([[-54.90068929],\n",
       "         [-16.30898967],\n",
       "         [154.700839  ]]),\n",
       "  array([[-53.96956374],\n",
       "         [-34.29186421],\n",
       "         [195.91561463]])))"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret, cameraMatrix, dist, rvecs, tvecs = cv2.calibrateCamera(objectPoints=objpoints, imagePoints=imgpoints, imageSize=size, cameraMatrix=None, distCoeffs=None)\n",
    "ret, cameraMatrix, dist, rvecs, tvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "53c62b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_path in images:\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    # scale_percent = 15\n",
    "    # w = int(img.shape[1] * scale_percent / 100)\n",
    "    # h = int(img.shape[0] * scale_percent / 100)\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    newCameraMatrix, roi = cv2.getOptimalNewCameraMatrix(cameraMatrix, dist, (w, h), 1, (w, h))\n",
    "\n",
    "    # undistort\n",
    "    dst = cv2.undistort(img, cameraMatrix, dist, None, newCameraMatrix)\n",
    "\n",
    "    # # crop the image\n",
    "    # x, y, w, h = roi\n",
    "    # dst = dst[y : y + h, x : x + w]\n",
    "    \n",
    "    name = img_path[img_path.rfind('\\\\')+1 : ]\n",
    "    cv2.imwrite(results_folder + name, dst)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be86a171",
   "metadata": {},
   "source": [
    "\n",
    "Nie wiem, czy dobrze działa ta kalibracja kamery (wyniki po undistort)\n",
    "\n",
    "Jakie listy zwracać w get_corresponding_points? Czym są argumenty objectPoints i imagePoints w cv2.calibrateCamera()?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pag",
   "language": "python",
   "name": "pag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
